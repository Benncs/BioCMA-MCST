#include <biocma_cst_config.hpp>
#include <cstddef>
#include <mc/domain.hpp>
#include <mc/events.hpp>
#include <simulation/simulation.hpp>
#include <sync.hpp>

#ifndef NO_MPI
#  include <mpi_w/wrap_mpi.hpp>
#else
using MPI_Request = int;
// MOCK
// TODO CLEAN FWD DECLARATION
// NOLINTBEGIN
namespace WrapMPI
{
  void barrier() {};
  template <typename T>
  [[maybe_unused]] std::vector<T> gather([[maybe_unused]] auto,
                                         [[maybe_unused]] auto,
                                         [[maybe_unused]] int = 0) {};

  template <typename T>
  [[maybe_unused]] void gather_span([[maybe_unused]] std::span<T>,
                                    [[maybe_unused]] std::span<const T>,
                                    [[maybe_unused]] size_t = 0) {};

  void broadcast_span([[maybe_unused]] auto data, [[maybe_unused]] auto n) {};
  template <typename T>
  [[maybe_unused]] int send_v([[maybe_unused]] std::span<const T> data,
                              [[maybe_unused]] size_t dest,
                              [[maybe_unused]] size_t tag,
                              [[maybe_unused]] bool send_size) noexcept {};

  // namespace Async
  // {

  //   template <typename T>
  //   [[maybe_unused]] int recv_span(MPI_Request& request, std::span<T> buf,
  //   size_t src, size_t tag) noexcept; void wait(MPI_Request& request);

  // } // namespace Async
} // namespace WrapMPI
// NOLINTEND
#endif

void sync_step(const ExecInfo& exec, Simulation::SimulationUnit& simulation)
{

  // Sync is not needed in shared mode because only one unit is used in this
  // case, furthermore, barrier is already handled by kokkos at the end of
  // cycling process
  if constexpr (AutoGenerated::FlagCompileTime::use_mpi)
  {
    PROFILE_SECTION("sync_step")
#ifndef NO_MPI
    WrapMPI::barrier();
    if (exec.current_rank == 0)
    {
      auto liquid_buffer =
          simulation.getContributionData_mut(); // Get the span of array on
                                                // which reduction happend
      MPI_Reduce(MPI_IN_PLACE,
                 liquid_buffer.data(),
                 liquid_buffer.size(),
                 MPI_DOUBLE,
                 MPI_SUM,
                 0,
                 MPI_COMM_WORLD);
    }
    else
    {
      const auto local_contribution =
          simulation.getContributionData(); // Get the span of array that are
                                            // add to reduction
      MPI_Reduce(local_contribution.data(),
                 nullptr,
                 local_contribution.size(),
                 MPI_DOUBLE,
                 MPI_SUM,
                 0,
                 MPI_COMM_WORLD);
    }
    WrapMPI::barrier();
#else
    (void)simulation;
    (void)exec;
#endif
  }
}

void sync_prepare_next(Simulation::SimulationUnit& simulation)
{
  PROFILE_SECTION("sync_prepare_next")
  simulation.clearContribution();
  if constexpr (AutoGenerated::FlagCompileTime::use_mpi)
  {
    // In multiple rank context, we also need to broadcast the updated liquid
    // concentration computed by the host during the current step

    auto data =
        simulation.getCliqData(); // Get concentration ptr wrapped into span

    WrapMPI::barrier();
    // We can use span here because we broadcast without changing size
    WrapMPI::broadcast_span(data, 0);
  }
}

void last_sync(const ExecInfo& exec, Simulation::SimulationUnit& simulation)
{
  // For the last synchronisation, the aim for the host rank is to retrive all
  // local information of worker ranks
  if constexpr (AutoGenerated::FlagCompileTime::use_mpi)
  {
    WrapMPI::barrier();

    // We first deal with event gathering, events can be broadcast and gather
    // easily as we deal with fixed sized array of integer

    if constexpr (AutoGenerated::FlagCompileTime::enable_event_counter)
    {
      auto& local_events = simulation.mc_unit->events;

      // We however have to declare events raw data as
      // vector to fit with WrapMPI API
      std::vector<size_t> total_events_data =
          WrapMPI::gather<size_t>(local_events.get_span(), exec.n_rank);

      if (exec.current_rank == 0)
      {
        // We could update 'local_events' for all ranks but it's useless as
        // simulation is finished and programm is going to exit
        local_events = MC::EventContainer::reduce(total_events_data);

        // TODO: MERGE PARTICLELIST
      }
      simulation.mc_unit->events =
          local_events; // FIX IT because we will reduce
                        // twice (here + post process)
    }
  }
}
